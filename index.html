<!DOCTYPE html>
<html lang="en">

<head>
  <br>
  <!-- Basic Page Needs
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>CompUDA</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/footable.standalone.min.css">

  <!-- Favicon
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" href="./img/logo.svg">

  <!-- Google icon -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0" />

  <!-- Hover effect: https://codepen.io/nxworld/pen/ZYNOBZ -->
  <style>
    img {
      display: block;
    }

    .column-50 {
      float: left;
      width: 50%;
    }

    .row-50:after {
      content: "";
      display: table;
      clear: both;
    }

    .floating-teaser {
      float: left;
      width: 30%;
      text-align: center;
      padding: 15px;
    }

    .venue strong {
      color: #99324b;
    }

    .benchmark {
      width: 100%;
      max-width: 960px;
      overflow: scroll;
      overflow-y: hidden;
    }
  </style>
</head>

<body>

  <!-- Primary Page Layout
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <h4 style="text-align:center">CompUDA: Compositional Unsupervised Domain Adaptation for Semantic Segmentation under Adverse Conditions</h4>
    <p align="center" , style="margin-bottom:12px;">
      <a class="simple" href="#" target="_blank">Ziqiang Zheng</a><sup>1</sup>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a class="simple" href="https://chenyingshu.github.io/" target="_blank">Yingshu Chen</a><sup>1</sup>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a class="simple" href="https://sonhua.github.io/" target="_blank">Binh-Son Hua</a><sup>2,3</sup>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a class="simple" href="https://saikit.org/" target="_blank">Sai-Kit Yeung</a><sup>1</sup>
    </p>

    <p align="center" style="margin-bottom:20px;">
      <sup>1</sup>The Hong Kong University of Science and Technology
      <br>
      <sup>2</sup>Trinity College Dublin
      <br>
      <sup>3</sup>VinAI Research
    </p>

    <div class="venue">
      <p align="center"><b>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023</b></p>
    </div>

    <figure>
      <img src="img/framework.png" style="width:100%"></img>
    </figure>
    
    <p align="center">Framework Overview of CompUDA. </p>
    <!-- The reference images collected under the normal condition (daytime) are regarded from the intermediate domain. We first perform the unsupervised domain adaptation (UDA) between the source domain and the intermediate domain and generated the pseudo labels for the daytime images. We assume the availability of image-level one-to-one correspondences (geometry alignment) between the daytime images and the low visibility images captured under various adverse conditions. We propose to use a geometry-aligned image translation to transfer the daytime images from the intermediate domain to a synthetic domain for reducing the visibility gap. We then conduct a final synthetic-to-real adaptation between the synthesized images and the target images. -->

    <div id="teaser" class="container" style="width:100%; margin:0; padding:0">

      <h5>Abstract</h5>
      <p align="justify">
         In autonomous driving, performing robust semantic segmentation under adverse weather conditions is a
          long-standing challenge. Imperfect camera observations under adverse conditions result in images with reduced visibility, which hinders label annotation and semantic scene understanding based on these images. A common solution is to adopt semantic segmentation models trained in a source domain with ground truth labels and perform unsupervised domain adaptation (UDA) from the source domain to an unlabeled target domain that has adverse conditions. Due to imperfect visual observations in the target domain, such adaptation needs special treatment to achieve good performance. In this paper, we propose a new compositional unsupervised domain adaptation (CompUDA) method that disentangles the domain gap based on multiple factors including style, visibility, and image quality. The domain gaps caused by these individual factors can then be addressed separately by introducing the intermediate domains. Specifically, 1) to address the style gap, we perform source-to-intermediate domain adaptation and generate pseudo-labels for self-training in the target domain; 2) to address the visibility gap, we perform a geometry-aligned normal-to-adverse image translation and introduce a synthetic domain; 3) finally, to address the image quality gap between the synthetic and target domain, we perform a synthetic-to-real adaptation based on the generated pseudo-labels. Our compositional unsupervised domain adaptation can be used in conjunction with a wide variety of semantic segmentation methods and result in significant performance improvement across datasets.
        <br>
        <br>
      </p>
    </div>


    <div class="section">
      <h5>Materials</h5>
      <div class="container" style="width:95%">
        <div class="row">
          <div class="three columns" style="display: flex; flex-direction: column; align-items: center;">
            <a href="./assets/iros2023_CompUDA.pdf" target="_blank">
              <span style="border: 1px solid #ddd; border-radius: 4px; padding: 2px; font-size: 108px;" class="material-symbols-outlined">
              article
              </span>
            </a>
            <a href="./assets/iros2023_CompUDA.pdf" target="_blank">Paper</a>
          </div>
          <div class="three columns" style="display: flex; flex-direction: column; align-items: center;">
            <a href="https://github.com/zhengziqiang/CompUDA" target="_blank">
              <span style="border: 1px solid #ddd; border-radius: 4px; padding: 2px; font-size: 108px;" class="material-symbols-outlined">
              code
              </span></a>
              <a href="https://github.com/zhengziqiang/CompUDA" target="_blank">Code</a>
          </div>
        </div>
      </div>
    </div> 

    <br>

    
   <!--  <div class="container" style="width:100%; margin:0; padding:0">
      <h5>Our Retrieval Module</h5>
      <center>
        <img src="images/retrieval_module.png" style="width:100%"></img>
        <div class="caption">
          <p align="justify">
            In the beginning, our Retrieval module will take <a href="https://mvk.hkustvgd.com/">MVK dataset</a> as input, then we pre-process videos to get CLIP features and metadata for indexing. With each text-based query, we will search and rank relevant videos and output them for use as input of the Explainability module. However, in order to make this process efficient and scalable, we split the retrieval module into two stages: the indexing stage and the retrieval stage. During the indexing stage, we preprocess and store the videos and their metadata in a vector database while maintaining all relevant information. In the retrieval stage, we use the power of both CPU and GPU to efficiently calculate the similarity scores between the query and the stored videos.
          </p>
        </div>
        <br>
      </center>
    </div> -->

    <div class="section">
      <h5>Citation</h5>
      <pre style="margin:0">
        <code>@inproceedings{zheng2023compuda,
          title={CompUDA: Compositional Unsupervised Domain Adaptation for Semantic Segmentation under Adverse Conditions},
          author={Ziqiang Zheng and Yingshu Chen and Binh-Son Hua and Sai-Kit Yeung},
          booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
          year={2023}
          organization={IEEE}
        }</code>
        </pre>
    </div>

    <!-- -->
    <br>

  <!-- End Document
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>

</html>
